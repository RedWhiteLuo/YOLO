# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""
import multiprocessing
from multiprocessing import Queue
import argparse
import os
import sys
import time
from pathlib import Path
import pydirectinput
import torch
import numpy as np
import win32gui
import win32ui
import win32con
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, smart_inference_mode
# ä»¥ä¸‹ä¸ºGRAB_screençš„åˆå§‹åŒ–å‚æ•°
xywh = [600, 180, 640, 512]
hwin = win32gui.GetDesktopWindow()
hwindc = win32gui.GetWindowDC(hwin)
srcdc = win32ui.CreateDCFromHandle(hwindc)
memdc = srcdc.CreateCompatibleDC()
bmp = win32ui.CreateBitmap()
bmp.CreateCompatibleBitmap(srcdc, xywh[2], xywh[3])
memdc.SelectObject(bmp)

# æ­¤å¤„éœ€è¦æ‰‹åŠ¨è¾“å…¥é•¿å®½ï¼ˆç”¨äºMOUSE_moveï¼‰
screen_w, screen_h = 640, 500


def IMG_show(img_list):
    while True:
        if img_list.full():     # å†—ä½™ï¼Œé˜²æ­¢å› å…¶ä»–åŸå› ä½¿è¿›ç¨‹é˜»å¡è¿›è€Œå¯¼è‡´æ¨ç†è¿‡ç¨‹æš‚åœ
            for x in range(4):
                img_list.get()
        cv2.imshow("THIS IS ASYNC RESULT", img_list.get())
        cv2.waitKey(1)  # 1 millisecond


def MOUSE_move(data):
    while True:
        choose_close = data.get()
        if choose_close[1] != 0:    # æ­¤å¤„æ˜¯ç”¨æ¥é˜²æ­¢å½’ä½å x,y éƒ½ç­‰äºâ€œ0â€çš„æƒ…å†µ
            pydirectinput.moveRel(xOffset=int(choose_close[1] - screen_w / 2),
                                  yOffset=int(choose_close[2] - screen_h / 2), relative=True)
            print("ASYNC[x y mouse_move_x mouse_move_y ]", choose_close[0], choose_close[1], choose_close[2],
                  int(choose_close[1] - (screen_w / 2)), int(choose_close[2] - (screen_h / 2)))
            # åˆ¤æ–­æ˜¯å¦åœ¨æŒ‡å®šçš„èŒƒå›´å†…ï¼Œå¦‚æ»¡è¶³åˆ™ç‚¹ä¸‹å·¦é”®
            if choose_close[0] <= 200:
                pydirectinput.click()
                # time.sleep(1.5)


def GRAB_screen():
    memdc.BitBlt((0, 0), (xywh[2], xywh[3]), srcdc, (xywh[0], xywh[1]), win32con.SRCCOPY)
    signedIntsArray = bmp.GetBitmapBits(True)
    img = np.frombuffer(signedIntsArray, np.uint8)
    img.shape = (xywh[3], xywh[2], 4)
    im0s = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)
    im = np.asarray(im0s)  # è½¬æ¢ä¸ºnp.arrayå½¢å¼[w,h,c]
    im = im.swapaxes(0, 2)  # äº¤æ¢ä¸º[c,h,w]
    im = im.swapaxes(1, 2)  # äº¤æ¢ä¸º[c,w,h]
    im[[0, 1, 2]] = im[[2, 1, 0]]  # å°†RGB è½¬æ¢ä¸º BGR
    return im, im0s

@smart_inference_mode()
def run(
        show_img,
        mouse_move,
        weights=ROOT / 'yolov5s.pt',  # model path or triton URL
        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)
        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path
        img_size=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=2,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img_sync=False,  # show results
        view_img_async=False,  # show results in async
        move_mouse_sync=False,
        move_mouse_async=False,
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
):
    source = str(source)

    # Load model
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    img_size = check_img_size(img_size, s=stride)  # check image size
    # Data loader
    bs = 1  # batch_size
    dataset = LoadScreenshots(source, img_size=img_size, stride=stride, auto=pt)
    # Run inference
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *img_size))  # warmup
    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())
    choose_close = [10000000000, 0, 0]
    end = time.perf_counter()
    '''for path, im, im0s, vid_cap, s in dataset:'''
    while True:
        im, im0s = GRAB_screen()
        s = "debug:"
        path = "0"
        start = time.perf_counter()
        time_count = start - end
        end = time.perf_counter()
        with dt[0]:
            im = torch.from_numpy(im).to(model.device)
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim
        # Inference
        with dt[1]:
            predict = model(im, augment=augment, visualize=False)
        # NMS
        with dt[2]:
            predict = non_max_suppression(predict, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Process predictions
        for i, det in enumerate(predict):  # per image
            seen += 1
            p, im0 = path, im0s.copy()
            s += '%gx%g ' % im.shape[2:]  # print string
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            annotator = Annotator(im0, line_width=int(2), example=str(names))
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    ppl_list = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()    # è¿”å›çš„æ˜¯åæ ‡æ¡†ä¸­å¿ƒçš„xywh
                    # å› ä¸ºæ²¡æœ‰è®­ç»ƒä¸“é—¨æ¨¡å‹ï¼Œæ­¤å¤„ç”¨æ¥æ’é™¤æ‰‹çš„å¹²æ‰°
                    if ppl_list[0] > 0.7 and ppl_list[1] > 0.7:
                        continue
                    # è·å¾—æ£€æµ‹æ¡†çš„ä¸­å¿ƒï¼Œä»¥åŠè¯¥ä¸­å¿ƒè·ç¦»å±å¹•ä¸­å¿ƒçš„è·ç¦»
                    ppl_x, ppl_y = int((ppl_list[0]) * screen_w), int((ppl_list[1] - ppl_list[3] * 0.25) * screen_h)
                    ppl_distance = int(ppl_x - screen_w / 2) ** 2 + int(ppl_y - screen_h / 2) ** 2      # ç”¨æ¥åˆ¤æ–­ä¸å¤´çš„è·ç¦»
                    # ç”¨æ¥ä¿å­˜åœ¨ä¸€å¼ å›¾ç‰‡ä¸­è·ç¦»ä¸­å¿ƒç‚¹æœ€è¿‘çš„åæ ‡æ¡†
                    if ppl_distance < choose_close[0]:
                        choose_close[0], choose_close[1], choose_close[2] = ppl_distance, ppl_x, ppl_y
                    # å¯¹å›¾ç‰‡è¿›è¡Œæ·»åŠ æ ‡ç­¾
                    if view_img_async or view_img_sync:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')  # æ ‡ç­¾
                        annotator.box_label(xyxy, label, color=colors(int(conf), True))  # åæ ‡ æ ‡ç­¾ é¢œè‰²ï¼ˆç½®ä¿¡åº¦ï¼‰
                    # ç§»åŠ¨é¼ æ ‡ï¼ˆè‡ªç„éƒ¨åˆ†ï¼Œè¿™ä¸€å—éœ€è¦ä½¿ç”¨ç®¡ç†å‘˜æƒé™ï¼Œå¹¶åœ¨æ¸¸æˆå†…æ‰“å¼€ï¼Œä¸ç„¶æœ‰å¯èƒ½ä¼šå¤±æ•ˆï¼‰
                if move_mouse_async:    # å¼‚æ­¥
                    mouse_move.put(choose_close)
                if move_mouse_sync:     # åŒæ­¥
                    if choose_close[1] != 0:
                        pydirectinput.moveRel(xOffset=int(choose_close[1] - screen_w / 2),
                                                  yOffset=int(choose_close[2] - screen_h / 2), relative=True)
                        print("SYNC[x y mouse_move_x mouse_move_y ]", choose_close[0], choose_close[1], choose_close[2],
                                int(choose_close[1] - (screen_w / 2)), int(choose_close[2] - (screen_h / 2)))
                        if choose_close[0] <= 200:
                            pydirectinput.click()
                            # time.sleep(1.5)
                choose_close = [10000000000, 0, 0]
            # Stream results
            if view_img_async:  # å¼‚æ­¥
                show_img.put(annotator.result())
            if view_img_sync:   # åŒæ­¥
                cv2.imshow("THIS IS SYNC RESULT", annotator.result())
                cv2.waitKey(1)  # 1 millisecond

        # Print time
        LOGGER.info(f"{s}{'' if len(det) else '(no detections), '}inference: {dt[1].dt * 1E3:.1f}ms, total: {time_count* 1E3:.2f}ms")

    # ç”±äºæ˜¯æ­»å¾ªç¯æ‰€ä»¥è¿™ä¸ªé€‰é¡¹æ²¡æœ‰ç”¨
    if update:
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')
    parser.add_argument('--source', type=str, default=ROOT / 'screen', help='file/dir/URL/glob/screen/0(webcam)')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--img-size', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img-sync', default=False, action='store_true', help='view-img-sync')
    parser.add_argument('--view-img-async', default=False, action='store_true', help='view-img-async')
    parser.add_argument('--move-mouse-sync', default=False, action='store_true', help='move-mouse-sync')
    parser.add_argument('--move-mouse-async', default=False, action='store_true', help='move-mouse-async')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.img_size *= 2 if len(opt.img_size) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


def main(show_img_0, mouse_move_0, opt_0):
    check_requirements(exclude=('tensorboard', 'thop'))
    run(show_img_0, mouse_move_0, **vars(opt_0))


if __name__ == "__main__":
    opt = parse_opt()
    show_img = Queue(5)
    mouse_move = Queue()
    MAIN = multiprocessing.Process(target=main, args=(show_img, mouse_move, opt))
    SHOW = multiprocessing.Process(target=IMG_show, args=(show_img,))
    MOVE = multiprocessing.Process(target=MOUSE_move, args=(mouse_move,))
    MAIN.start()
    SHOW.start()
    MOVE.start()


'''
æµ‹è¯•æ•°æ®ï¼ˆ3060 Laptopï¼Œi7 12700hï¼‰ï¼š

##########################################æ˜¾ç¤ºå›¾ç‰‡#################################################
æ— ç›®æ ‡æ—¶(æ— é¼ æ ‡ç§»åŠ¨)(è°ƒç”¨win32apiæˆªå›¾):
    å¼‚æ­¥æ˜¾ç¤ºå›¾ç‰‡:
        debug:512x640 (no detections), inference: 6.6ms, total: 20.23ms
        debug:512x640 (no detections), inference: 6.6ms, total: 20.77ms
        debug:512x640 (no detections), inference: 9.9ms, total: 14.29ms
        debug:512x640 (no detections), inference: 6.0ms, total: 20.88ms
    åŒæ­¥æ˜¾ç¤ºå›¾ç‰‡:
        debug:512x640 (no detections), inference: 9.4ms, total: 28.12ms
        debug:512x640 (no detections), inference: 9.3ms, total: 27.71ms
        debug:512x640 (no detections), inference: 9.3ms, total: 34.73ms
        debug:512x640 (no detections), inference: 9.3ms, total: 28.24ms
        
##########################################ç§»åŠ¨é¼ æ ‡################################################
æ— å®æ—¶é¢„è§ˆæ—¶(3ä¸ªç›®æ ‡)(å…³é—­é¼ æ ‡æš‚åœ)
    åŒæ­¥ç§»åŠ¨é¼ æ ‡
         SYNC[x y mouse_move_x mouse_move_y ] 305 304 243 -16 -7
        screen 0 (LTWH): 640,304,640,512: 512x640 4 persons, inference: 12.5ms, total: 128.05ms
            SYNC[x y mouse_move_x mouse_move_y ] 305 304 243 -16 -7
        screen 0 (LTWH): 640,304,640,512: 512x640 4 persons, inference: 6.0ms, total: 112.29ms
            SYNC[x y mouse_move_x mouse_move_y ] 305 304 243 -16 -7
        screen 0 (LTWH): 640,304,640,512: 512x640 4 persons, inference: 12.0ms, total: 124.37ms
            SYNC[x y mouse_move_x mouse_move_y ] 305 304 243 -16 -7
        screen 0 (LTWH): 640,304,640,512: 512x640 4 persons, inference: 23.9ms, total: 145.60ms
    å¼‚æ­¥ç§»åŠ¨é¼ æ ‡
        æš‚æ—¶ä¸è€ƒè™‘ï¼Œpydirectinput æ•ˆç‡ä¼¼ä¹æ¯”è¾ƒä½ä¸‹ï¼Œä¼šæœ‰å¾ˆæ˜æ˜¾çš„æ»å

##########################################å±å¹•æˆªå›¾####################################################
    ä½¿ç”¨yoloè‡ªå¸¦çš„å±å¹•è·å–(æ‹–åŠ¨æ£®æ—å›¾ç‰‡ç§»åŠ¨)
        screen 0 (LTWH): 640,304,640,512: 512x640 (no detections), inference: 13.5ms, total: 21.08ms
        screen 0 (LTWH): 640,304,640,512: 512x640 (no detections), inference: 8.9ms, total: 27.76ms
        screen 0 (LTWH): 640,304,640,512: 512x640 (no detections), inference: 10.1ms, total: 20.67ms
        screen 0 (LTWH): 640,304,640,512: 512x640 (no detections), inference: 8.4ms, total: 20.55ms
    è°ƒç”¨win32apiè·å–å±å¹•(æ‹–åŠ¨æ£®æ—å›¾ç‰‡ç§»åŠ¨)
        debug:512x640 (no detections), inference: 8.9ms, total: 21.25ms
        debug:512x640 (no detections), inference: 11.7ms, total: 19.96ms
        debug:512x640 (no detections), inference: 8.9ms, total: 20.19ms
        debug:512x640 (no detections), inference: 10.5ms, total: 20.80ms
'''
